{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PredictAminos.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmZ5uzQUOvPR"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import statistics\n",
        "import tensorflow as tf\n",
        "import string\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from google.colab import drive\n",
        "from tensorflow.python.framework import ops\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "############# the code for this class came from Google's RNN Text Generation page but has been modified to work witht he current RNN ###############\n",
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  #@tf.function\n",
        "  def generate_one_step(self, inputs):\n",
        "\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # convert the input into one hot tensors\n",
        "    input_ids = tf.one_hot(input_ids,21)\n",
        "    input_ids = ops.convert_to_tensor(input_ids, dtype=tf.float32)\n",
        "\n",
        "\n",
        "    # Run the model.\n",
        "    predicted_logits = self.model(inputs=input_ids)\n",
        "    \n",
        "    \n",
        "    # Only use the last prediction.\n",
        "\n",
        "    predicted_logits = predicted_logits / self.temperature\n",
        "\n",
        "\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "    \n",
        "\n",
        "    # Return the characters.\n",
        "    return predicted_chars\n",
        "\n",
        "##### end of class #####\n",
        "\n",
        "model = keras.models.load_model('/content/gdrive/My Drive/sample1')\n",
        "\n",
        "\n",
        "# proteins for trial\n",
        "protein_seq = \"MVLSEGEWQLVLHVWAKVEADVAGHGQDILIRAEKLFNQDVDAAVRGILR\"\n",
        "protein_seq2 = \"MPPYTVVYFPVRGRCAALRMLLADQGQSWKEEVVTVETWQEGSLKASCLY\"\n",
        "protein_seq3 = \"KVFERCELARTLKRLGMDGYRGISLANWMCLAKWESGYNTRATNYNAGDR\"\n",
        "protein_seq4 = \"FNASSGDSKKIVGVFYKANEYATKNPNFLGCVENALGIRDWLESQGHQYI\"\n",
        "protein_seq5 = \"MDSEVQRDGRILDLIDDAWREDKLPYEDVAIPLNELPEPEQDNGGTTESV\"\n",
        "\n",
        "# protein to get vocabulary\n",
        "example_protein = \"METKTLIVNGMARRLLVSPNDLLVDVLRSQLQLTSVKVGCGKGQCGACTVILDGKVVRACIIKMSRVAENASVTTLEGIGAPDCLHPLQHAWIQHGAAQCGFCTPGFIVSAKALLDENVAPSREDVRDWFQKHHNICRCTGYKPLVDAVMDAAAILRGEKTVEEISFKMPADGRIWGSSIPRPSAVAKVTGLAEFGADAALRMPENTLHLALAQAKVSHALIKGIDTSEAEKMPGVYKVLTHKDVKGKNRITGLITFPTNKGDGWERPILNDSKIFQYGDALAIVCADSEANARAAAEKVKFDLELLPEYMSAPEAMAPDAIEIHPGTPNVYYDQLEEKGEDTVPFFNDPANVVAEGSYYTQRQPHLPIEPDVGYGYINEQGQVVIHSKSVAIHLHALMIAPGLGLEFPKDLVLVQNTTGGTFGYKFSPTMEALVGVAVMATGRPCHLRYNYEQQQNYTGKRSPFWTTMRYAADRQGKILAMETDWSVDHGPYSEFGDLLTLRGAQYIGAGYGIANIRGTGRTVATNHCWGAAFRGYGAPESEFPSEVLMDELAEKLGMDPFELRALNCYREGDTTSSGQIPEVMSLPEMFDKMRPYYEESKKRVKERSTAEIKRGVGVALGVYGAGLDGPDTSEAWVELNDDGSVTLGNSWEDHGQGADAGSLGTAHEALRPLGITPENIHLVMNDTSKTPNSGPAGGSRSQVVTGNAIRVACEMLIEGMRKPGGGFFTPAEMKAEGRPMRYDGKWTAPAKDCDAKGQGSPFACYMYGLFLTEVAVEVATGKATVEKMVCVADIGKICNKLVVDGQIYGGLAQGVGLALSEDYEDLKKHSTMGGAGIPSIKMIPDDIEIVYVETPRKDGPFGASGVGEMPLTAPHAAIINGIYNACGARVRHLPARPEKVLEAMPR\"\n",
        "\n",
        "# getting the vocabulary of the protein sequence as well as their associated IDs\n",
        "vocab = sorted(set(example_protein))\n",
        "ids_from_chars = preprocessing.StringLookup(vocabulary=list(vocab), mask_token=None)\n",
        "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
        "\n",
        "# get the one step modelclass initialized so prediction can be performed\n",
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
        "\n",
        "\n",
        "# preparing trials\n",
        "trials = 1\n",
        "k = 1\n",
        "i = 0\n",
        "array_of_proteins = []\n",
        "array_of_proteins.append(protein_seq)\n",
        "array_of_proteins.append(protein_seq2)\n",
        "array_of_proteins.append(protein_seq3)\n",
        "array_of_proteins.append(protein_seq4)\n",
        "array_of_proteins.append(protein_seq5)\n",
        "\n",
        "\n",
        "\n",
        "#array_of_proteins = np.array(array_of_proteins)\n",
        "\n",
        "# beginning trials\n",
        "while trials < 6:\n",
        "  print(\"\\nBeginning trial \" + str(trials))\n",
        "  print(\"===============================================================\")\n",
        "  print(\"===============================================================\\n\")\n",
        "  ar = array_of_proteins[i]\n",
        "\n",
        "  while k != 20:\n",
        "    chars = ar[:k]\n",
        "    next_char = tf.constant([chars])\n",
        "    result = []\n",
        "    result.append(chars)\n",
        "    next_letter = []\n",
        "\n",
        "    for n in range(350-k):\n",
        "      next_letter = one_step_model.generate_one_step(next_char)\n",
        "      next_letter_np = next_letter.numpy()\n",
        "      result.append(next_letter_np[0])\n",
        "\n",
        "    print(\"When k = \" + str(k))\n",
        "    print(\"-\"*len(result))\n",
        "    #k += 1\n",
        "\n",
        "    print(\"\\n-----------Finding matches-----------\\n\")\n",
        "    print(\"Prediction with seed of \" + str(k))\n",
        "    matches = 0\n",
        "    checkMatches = ar[k:]\n",
        "    k += 1\n",
        "\n",
        "    for x in range(len(checkMatches)):\n",
        "      if checkMatches[x].encode(\"utf-8\") == result[x]:\n",
        "        matches += 1\n",
        "      else:\n",
        "        continue\n",
        "\n",
        "    print(str(matches) + \" matches\")\n",
        "    print(\"________________________\\n\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "  # end of for loop and going on to the next rial  \n",
        "  i += 1\n",
        "  k = 1\n",
        "  trials += 1 \n",
        "\n",
        "print(\"\\n End of trials.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}